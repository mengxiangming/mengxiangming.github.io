<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="icon.png">
  <title>Xiangming Meng</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Xiangming Meng 孟 祥明</name>
              </p>
              <p>
              I am an Assistant Professor at The Zhejiang University-University of Illinois Urbana-Champaign Institute (<a href="https://zjui.intl.zju.edu.cn/en">ZJUI</a>), Zhejiang University. Before that, I was a Project Assistant Professor at the <a href="https://kaba-lab.org/en/">Kabashima Lab</a>  in the <a href="https://www.phys.s.u-tokyo.ac.jp/en/lp/ipi/">Institute for Physics of Intelligence (i &#960)</a>, <a href="https://www.u-tokyo.ac.jp/en">The University of Tokyo (UTokyo)</a> from April 2022 to March 2023. I completed my Ph.D. in 2016 from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, supervised by <a href="https://www.tsinghua.edu.cn/info/1167/93854.htm">Jianhua Lu</a>. I received a B.E. from <a href="https://en.xidian.edu.cn">Xidian University</a> in 2011.
              Previously I was  a postdoctoral researcher in <a href="https://www.phys.s.u-tokyo.ac.jp/en/lp/ipi/"> i &#960 </a> , <a href="https://www.u-tokyo.ac.jp/en">UTokyo</a> under supervision of  <a href="https://researchmap.jp/yoshiyuki.kabashima?lang=en"> Yoshiyuki Kabashima</a> from April 2020 to March 2022, and
                a postdoctoral researcher in the <a href="https://team-approx-bayes.github.io">Approximate Bayesian Inference Team</a>,  RIKEN center for Advanced Intelligence Project  (<a href="https://www.cs.washington.edu/"> RIKEN-AIP</a>) under supervision of  <a href="https://emtiyaz.github.io"> Emtiyaz Khan</a> from July 2019 to March 2020. I worked as a senior research engineer at  <a href="https://www.huawei.com/en/">Huawei Technologies Co., Ltd. </a>   from July 2016 to June 2019. I also visited University of Illinois Urbana-Champaign Institute (UIUC) from August 2023 to December 2023, hosted by  <a href="https://ece.illinois.edu/about/directory/faculty/z-liang">Zhi-Pei Liang</a>.   
              I am broadly interested in the intersection of machine learning, information theory, and statistical mechanics, with a special focus on graphical models, Bayesian inference, and learning algorithms.
              </p>

              <p align=center>
<!--				  <a href="https://drive.google.com/file/d/1zwjK26dgouy9diFvVhvJ1fTsoCzi87j_/view?usp=sharing">CV</a> &nbsp/&nbsp-->
                <a href="mailto:xiangmingmeng@intl.zju.edu.cn">Email</a> &nbsp/&nbsp
<!--                <a href="https://www.gshi.me/blog/">Blog</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=oV70ZoQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
               <a href="https://github.com/mengxiangming">Github</a> &nbsp/&nbsp
               <a href="https://researchmap.jp/xiangming.meng?lang=en">Researchmap</a>  &nbsp/&nbsp
                <a href="https://person.zju.edu.cn/xiangmingmeng">ZJU page</a>  
         
                
<!--                <a href="https://drive.google.com/file/d/1jKnpOO7L5J_y0_HGcvpqSm0fBJcO4OBM/view?usp=sharing"> Research Statement </a>-->
              </p>
            </td>
            <td width="33%">
              <img src="photo/meng.jpg" width='100%'>
            </td>
          </tr>
        </table>
        
        <hr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p>
                <strong style="color:black;">News:</strong>  I was invited to serve as an Area Chair for <a href="https://iclr.cc/">ICLR 2025</a>. 
              <p>
              
              <p>
                <strong style="color:black;">News:</strong>  One paper was accepted by IJCAI 2024</a>. 
              <p>

              <p>
                <strong style="color:black;">News:</strong>  I was invited to serve as an Area Chair for <a href="https://neurips.cc/">NeurIPS 2024</a>. 
              <p>

                
              <p>
                <strong style="color:black;">News:</strong> Our paper  <a href="https://arxiv.org/abs/2302.00919">QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models</a>  was accepted by AAAI 2024.
              <p>
              <p>
                
               <p>
                <strong style="color:black;">News:</strong> I am looking for postdoctoral researchers and research assistants. Please send me your detailed CV if you are interested. 
              <p>
              <p>
                <strong style="color:black;">News:</strong> I joined <a href="https://zjui.intl.zju.edu.cn/en">ZJUI</a>  as a tensure-track assistant professor from March 20th, 2023.
              <p>
              <p>
                <strong style="color:black;">News:</strong> Our paper <a href="https://openreview.net/forum?id=OOWLRfAI_V_">Quantized Compressed Sensing with Score-Based Generative Models</a>  was accepted by ICLR 2023.
              <p>
                <strong style="color:black;">News:</strong> Our paper <a href="https://arxiv.org/abs/2110.08500">On Model Selection Consistency of Lasso for High-Dimensional Ising Models</a>  was accepted by AISTATS 2023.
              <p>
                <strong style="color:black;">News:</strong> Our paper <a href="https://arxiv.org/abs/2302.13093">Average case analysis of Lasso under ultra sparse conditions</a>   was accepted by AISTATS 2023.                
             <p>
                <strong style="color:black;">News:</strong> Our paper <a href="https://openreview.net/forum?id=2KQ_ChYnEP9">Exact Solutions of a Deep Linear Network</a>  was accepted by NeurIPS 2022.
              </p>
              </p>    
            </td>
          </tr>
        </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Open Positions</heading>
              <br>
              <br>
              <p> I am always looking for highly motivated postdoctoral researchers and research assistants with a great passion for doing research in machine learning, signal processing, wireless communication, and other related fields. Please send your detailed CV (including education background, publication list, and research interests) to the email address above if you are interested. 
            </td>
          </tr>
        </table>
  
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research and Selected Publications</heading>
              <br>
              <br>
              <p> My research interests lie at the intersection of  machine learning, information theory and statistical mechanics, with an exploration of common principles within different fields.  Specific focuses are graphical models, Bayesian inference, and learning algorithms.
              <p>For an up-to-date publication list, please see the <a href="https://scholar.google.com/citations?user=oV70ZoQAAAAJ&hl=en">Google Scholar</a> page. (*correspondence)</p>
            </td>
          </tr>
        </table>

<!--Use the following form if groups are used for different publications-->
<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--          <tr>-->
<!--            <td width="85%" valign="left">-->
<!--              <papertitle><em>Neural-Control</em> Family: Stable and Robust Deep-learning-based Nonlinear Control in Dynamic Environments</papertitle> [<a href="https://www.gshi.me/blog/NeuralControl/">blog post</a>]-->
<!--              <hr>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </table>-->

  
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="width:35%;vertical-align:middle">
              <img src='/images/QCS-SGM-plus.png' width=100%>
            </td>
            <td style="width:65%;vertical-align:middle">
              <p>
                <a href="https://arxiv.org/abs/2302.00919v2">
                  <papertitle>QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models</papertitle>
                </a>
                <br>
                <strong>Xiangming Meng<sup>*</sup></strong> and  <a href="https://scholar.google.com.hk/citations?user=NLBZuoEAAAAJ&hl=zh-CN">Yoshiyuki Kabashima</a>
<!--                 <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021. -->
                <br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29347">AAAI2024</a>]
                [<a href="https://arxiv.org/abs/2302.00919v4">arXiv</a>]
                [<a href="https://github.com/mengxiangming/QCS-SGM-plus">code</a>]                
                <br>
                <p></p>
                <p> In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein an expectation propagation algorithm is employed for its approximate computation. We conduct extensive experiments on various settings, demonstrating the substantial superiority of QCS-SGM+ over QCS-SGM for general sensing matrices beyond mere row-orthogonality.
                </p>
            </td>
          </tr>

        </table>
  
  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="width:35%;vertical-align:middle">
              <img src='/images/dmps_cover.png' width=100%>
            </td>
            <td style="width:65%;vertical-align:middle">
              <p>
                <a href="https://arxiv.org/abs/2211.12343">
                  <papertitle>Diffusion Model Based Posterior Samplng for Noisy Linear Inverse Problems</papertitle>
                </a>
                <br>
                <strong>Xiangming Meng<sup>*</sup></strong> and  <a href="https://scholar.google.com.hk/citations?user=NLBZuoEAAAAJ&hl=zh-CN">Yoshiyuki Kabashima</a>
<!--                 <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021. -->
                <br>
                [<a href="https://www.acml-conf.org/2024/">ACML</a>]
                [<a href="https://arxiv.org/abs/2211.12343">arXiv</a>]
                [<a href="https://github.com/mengxiangming/dmps">code</a>]                
                <br>
                <p></p>
                <p> We consider the ubiquitous linear inverse problems with additive Gaussian noise and propose an unsupervised general-purpose sampling approach called diffusion model based posterior sampling (DMPS) to reconstruct the unknown signal from noisy linear measurements. Specifically, the prior of the unknown signal is implicitly modeled by one pre-trained diffusion model (DM). In posterior sampling, to address the intractability of exact noise-perturbed likelihood score, a simple yet effective noise-perturbed pseudo-likelihood score is introduced under the uninformative prior assumption. While DMPS applies to any kind of DM with proper modifications, we focus on the ablated diffusion model (ADM) as one specific example and evaluate its efficacy on a variety of linear inverse problems such as image super-resolution, denoising, deblurring, colorization. Experimental results demonstrate that, for both in-distribution and out-of-distribution samples,  DMPS achieves highly competitive or even better performances on various tasks while being 3 times  faster than the leading competitor.
                </p>
            </td>
          </tr>

        </table>
  
  
  
  
  
  
  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="width:35%;vertical-align:middle">
              <img src='/images/qcs-sgm-cover.png' width=100%>
            </td>
            <td style="width:65%;vertical-align:middle">
              <p>
                <a href="https://arxiv.org/abs/2211.13006">
                  <papertitle>Quantized Compressed Sensing with Score-Based Generative Models</papertitle>
                </a>
                <br>
                <strong>Xiangming Meng<sup>*</sup></strong> and  <a href="https://scholar.google.com.hk/citations?user=NLBZuoEAAAAJ&hl=zh-CN">Yoshiyuki Kabashima</a>
<!--                 <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021. -->
                <br>
                [<a href="https://openreview.net/forum?id=OOWLRfAI_V_">ICLR2023</a>]
                [<a href="https://arxiv.org/abs/2211.13006">arXiv</a>]
                [<a href="https://github.com/mengxiangming/QCS-SGM">code</a>]                
<!--                 [<a href="https://proceedings.neurips.cc/paper/2021/hash/31917677a66c6eddd3ab1f68b0679e2f-Abstract.html">NeurIPS</a>] -->
<!--                 [<a href="https://slideslive.com/38968645/ising-model-selection-using-l1regularized-linear-regression-a-statistical-mechanics-analysis?ref=recommended">video</a>] -->
<!--                 [<a href="https://neurips.cc/media/neurips-2021/Slides/28748.pdf">slides</a>] -->
                <br>
                <p></p>
                <p> We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called noise perturbed pseudo-likelihood score is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. 
                </p>
            </td>
          </tr>

        </table>
  
  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="width:35%;vertical-align:middle">
              <img src='/images/aistats2023cover.png' width=100%>
            </td>
            <td style="width:65%;vertical-align:middle">
              <p>
                <a href="https://arxiv.org/abs/2102.03988">
                  <papertitle>On Model Selection Consistency of Lasso for High-Dimensional Ising Models</papertitle>
                </a>
                <br>
                <strong>Xiangming Meng<sup>*</sup></strong>, <a href="https://scholar.google.com.hk/citations?user=AYqKJtcAAAAJ&hl=zh-CN">Tomoyuki Obuchi</a>, <a href="https://scholar.google.com.hk/citations?user=NLBZuoEAAAAJ&hl=zh-CN">Yoshiyuki Kabashima</a>
                <br>
                <em>The 26th International Conference on Artificial Intelligence and Statistics (AISTATS) </em>, 2023.
                <br>
                [<a href="https://arxiv.org/abs/2110.08500">arXiv</a>]
                [<a href="http://aistats.org/aistats2023/accepted.html">AISTATS</a>]
                <br>
                <p></p>
                <p> We theoretically analyze the model selection consistency of least absolute shrinkage and selection operator (Lasso) for high-dimensional Ising models. For general tree-like graphs, it is rigorously proved that Lasso without post-thresholding is model selection consistent in the whole paramagnetic phase with the same order of sample complexity  as that of L1-regularized logistic regression (L1-LogR). This result is consistent with the conjecture in Meng, Obuchi, and Kabashima 2021 using the non-rigorous replica method from statistical physics and thus complements it with a rigorous proof. Moreover, we provide a rigorous proof of the model selection consistency of Lasso with post-thresholding for general tree-like graphs in the paramagnetic phase without further assumptions on the dependency and incoherence conditions.
                </p>
            </td>
          </tr>

        </table>
  
  
  

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td style="width:35%;vertical-align:middle">
              <img src='/images/NeurIPS2021_paper.png' width=100%>
            </td>
            <td style="width:65%;vertical-align:middle">
              <p>
                <a href="https://arxiv.org/abs/2102.03988">
                  <papertitle>Ising Model Selection Using L1-Regularized Linear Regression: A Statistical Mechanics Analysis</papertitle>
                </a>
                <br>
                <strong>Xiangming Meng<sup>*</sup></strong>, <a href="https://scholar.google.com.hk/citations?user=AYqKJtcAAAAJ&hl=zh-CN">Tomoyuki Obuchi</a>, <a href="https://scholar.google.com.hk/citations?user=NLBZuoEAAAAJ&hl=zh-CN">Yoshiyuki Kabashima</a>
                <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021.
                <br>
                [<a href="https://arxiv.org/abs/2102.03988">arXiv</a>]
                [<a href="https://proceedings.neurips.cc/paper/2021/hash/31917677a66c6eddd3ab1f68b0679e2f-Abstract.html">NeurIPS</a>]
                [<a href="https://slideslive.com/38968645/ising-model-selection-using-l1regularized-linear-regression-a-statistical-mechanics-analysis?ref=recommended">video</a>]
                [<a href="https://neurips.cc/media/neurips-2021/Slides/28748.pdf">slides</a>]
                <br>
                <p></p>
                <p> We theoretically investigate the typical learning performance of L1-regularized linear regression (L1-LinR, i.e., Lasso) for Ising model selection using the replica method from statistical mechanics. We obtain an accurate estimate of the typical sample complexity of L1-LinR, which demonstrates that L1-LinR is model selection consistent with M=0(log N) samples, where N is the number of variables of the Ising model. Moreover, we provide a computationally efficient method to accurately predict the non-asymptotic behavior of L1-LinR for moderate M and N, such as the precision and recall rates.
                </p>
            </td>
          </tr>

        </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="width:35%;vertical-align:middle">
                      <img src='/images/ICML2020_paper.png' width=100%>
                  </td>
                  <td style="width:65%;vertical-align:middle">
                      <p>
                          <a href="https://arxiv.org/abs/2002.10778">
                              <papertitle>Training Binary Neural Networks using the Bayesian Learning Rule</papertitle>
                          </a>
                          <br>
                          <strong>Xiangming Meng<sup></strong>, <a href="https://scholar.google.co.jp/citations?user=-KHAy7kAAAAJ&hl=ja">Roman Bachmann</a>, <a href="https://emtiyaz.github.io">Mohammad Emtiyaz Khan</a>*</sup>
                          <br>
                          <em>The Thirty-seventh International Conference on Machine Learning (ICML)</em>, 2020.
                          <br>
                          [<a href="https://arxiv.org/abs/2002.10778">arXiv</a>]
                          [<a href="http://proceedings.mlr.press/v119/meng20a.html">ICML</a>]
                          [<a href="https://slideslive.com/38928553/training-binary-neural-networks-using-the-bayesian-learning-rule?ref=speaker-17205">video</a>]
                          [<a href="https://icml.cc/media/icml-2020/Slides/6823.pdf">slides</a>]
                          [<a href="https://github.com/team-approx-bayes/BayesBiNN">code</a>]
                        
                          <br>
                      <p></p>
                      <p> Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation and continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which also justifies and extends existing approaches.                      </p>
                  </td>
              </tr>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="width:35%;vertical-align:middle">
                      <img src='/images/JSAC2021_paper.png' width=100%>
                  </td>
                  <td style="width:65%;vertical-align:middle">
                      <p>
                          <a href="https://ieeexplore.ieee.org/abstract/document/9181630/">
                              <papertitle>Advanced NOMA Receivers From a Unified Variational Inference Perspective</papertitle>
                          </a>
                          <br>
                          <strong>Xiangming Meng<sup></strong>, Lei Zhang, Chao Wang, Lei Wang, Yiqun Wu, <a href="https://scholar.google.com/citations?user=aYSn_sQAAAAJ&hl=zh-CN">Yan Chen</a>*</sup>, <a href="https://scholar.google.co.uk/citations?user=ousWjhgAAAAJ&hl=en">Wenjin Wang</a>
                          <br>
                          <em>IEEE Journal on Selected Areas in Communications  (JSAC)</em>, 2021.
                          <br>
<!--                          [<a href="https://arxiv.org/abs/2002.10778">arXiv</a>]-->
                          [<a href="https://ieeexplore.ieee.org/abstract/document/9181630">IEEE</a>]
<!--                          [<a href="https://slideslive.com/38928553/training-binary-neural-networks-using-the-bayesian-learning-rule?ref=speaker-17205">video</a>]-->
<!--                          [<a href="https://icml.cc/media/icml-2020/Slides/6823.pdf">slides</a>]-->
                          <br>
                      <p></p>

                      <p> Non-orthogonal multiple access (NOMA) on shared resources has been identified as a promising technology in 5G to improve resource efficiency and support massive access in all kinds of transmission modes. Power domain and code domain NOMA have been extensively studied and evaluated in both literatures and 3GPP standardization, especially for the uplink where large number of users would like to send their messages to the base station. Though different in the transmitter side design, power domain NOMA and code domain NOMA share the same need of the advanced multi-user detection (MUD) design at the receiver side. Various multi-user detection algorithms have been proposed, balancing performance and complexity in different ways, which is important for the implementation of NOMA in practical networks. In this paper, we introduce a unified variational inference (VI) perspective on various universal NOMA MUD algorithms such as belief propagation (BP), expectation propagation (EP), vector EP (VEP), approximate message passing (AMP) and vector AMP (VAMP), demonstrating how they could be derived from and adapted to each other within the VI framework. Moreover, we unveil and prove that conventional elementary signal estimator (ESE) and linear minimum mean square error (LMMSE) receivers are special cases of EP and VEP, respectively, thus bridging the gap between classic linear receivers and message passing based nonlinear receivers. Such a unified perspective would not only help the design and adaptation of NOMA receivers, but also open a door for the systematic design of joint active user detection and multi-user decoding for sporadic grant-free transmission.

                  </td>
              </tr>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="width:35%;vertical-align:middle">
                      <img src='/images/SPL_2018_paper.png' width=100%>
                  </td>
                  <td style="width:65%;vertical-align:middle">
                      <p>
                          <a href="https://ieeexplore.ieee.org/abstract/document/8244269/">
                              <papertitle>A unified Bayesian Inference Framework for Generalized Linear Models</papertitle>
                          </a>
                          <br>
                          <strong>Xiangming Meng</strong>*</sup>, <a href="https://scholar.google.com.hk/citations?user=qwz0E6MAAAAJ&hl=zh-CN">Sheng Wu</a>, <a href="https://scholar.google.com.hk/citations?user=kAjruiMAAAAJ&hl=zh-CN">Jiang Zhu</a>
                          <br>
                          <em>IEEE Signal Processing Letters  (SPL)</em>, March 2018.
                          <br>
                          [<a href="https://arxiv.org/abs/1712.10288">arXiv</a>]
                          [<a href="https://ieeexplore.ieee.org/abstract/document/8244269/">IEEE</a>]
                          [<a href="https://github.com/mengxiangming/glmcode">code</a>]
                          <!--                          [<a href="https://slideslive.com/38928553/training-binary-neural-networks-using-the-bayesian-learning-rule?ref=speaker-17205">video</a>]-->
                          <!--                          [<a href="https://icml.cc/media/icml-2020/Slides/6823.pdf">slides</a>]-->
                          <br>
                      <p></p>

                      <p> Based on expectation propagation (EP), we present a unified Bayesian inference framework for generalized linear models (GLM) which iteratively reduces the GLM problem to a sequence of standard linear model (SLM) problems.
                          This framework provides new perspectives on some existing GLM algorithms and also suggests novel extensions for some other SLM algorithms. Specific instances elucidated under such framework are the GLM versions of approximate message passing (AMP), vector AMP (VAMP), and sparse Bayesian learning (SBL). In particular, we provide an EP perspective on the famous generalized approximate message passing (GAMP) algorithm, which leads to  a concise derivation of GAMP via EP.


                  </td>
              </tr>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="width:35%;vertical-align:middle">
                      <img src='/images/SPL_2015_paper.png' width=100%>
                  </td>
                  <td style="width:65%;vertical-align:middle">
                      <p>
                          <a href="https://ieeexplore.ieee.org/abstract/document/7008468">
                              <papertitle>An Expectation Propagation Perspective on Approximate Message Passing</papertitle>
                          </a>
                          <br>
                          <strong>Xiangming Meng</strong>*</sup>, <a href="https://scholar.google.com.hk/citations?user=qwz0E6MAAAAJ&hl=zh-CN">Sheng Wu</a>, <a href="https://scholar.google.com/citations?user=9BioD-kAAAAJ&hl=zh-CN">Linling Kuang</a>, <a href="https://www.tsinghua.edu.cn/info/1167/93854.htm">Jianhua Lu</a>            
                          <br>
                          <em>IEEE Signal Processing Letters  (SPL)</em>, August 2015.
                          <br>
<!--                          [<a href="https://arxiv.org/abs/1712.10288">arXiv</a>]-->
                          [<a href="https://ieeexplore.ieee.org/abstract/document/7008468">IEEE</a>]
<!--                          [<a href="https://github.com/mengxiangming/glmcode">code</a>]-->
                          <!--                          [<a href="https://slideslive.com/38928553/training-binary-neural-networks-using-the-bayesian-learning-rule?ref=speaker-17205">video</a>]-->
                          <!--                          [<a href="https://icml.cc/media/icml-2020/Slides/6823.pdf">slides</a>]-->
                          <br>
                      <p></p>

                      <p> An alternative derivation for the well-known approximate message passing (AMP) algorithm proposed by Donoho is presented in this letter. Compared with the original derivation, which exploits central limit theorem and Taylor expansion to simplify belief propagation (BP), our derivation resorts to expectation propagation (EP) and the neglect of high-order terms in large system limit. This alternative derivation leads to a different yet provably equivalent form of message passing, which explicitly establishes the intrinsic connection between AMP and EP, thereby offering some new insights in the understanding and improvement of AMP.

                  </td>
              </tr>

          </table>
        <hr>

<!--This part adds contents of Teaching -->
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="100%" valign="middle">
                      <heading>Teaching</heading>
                  </td>
              </tr>
          </table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      <p style="font-size: 18px; font-weight: bold;">Undergraduate Courses:</p>
      <ul style="font-size: 16px;">
        <li><strong>ECE210:</strong> Analog Signal Processing. Spring 2024.</li>
        <li><strong>Math257:</strong> Linear Algebra with Computational Applications. Fall 2024.</li>
        <li><strong>ECE313:</strong> Probability with Engineering Applications. Spring 2025.</li>
      </ul>
    </td>
  </tr>
</table>


  

<!--This part adds contents of invited talks-->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td width="100%" valign="middle">
                      <heading>Invited Talks</heading>
                  </td>
              </tr>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="width:65%;vertical-align:middle">
                      <p>


                          <a href="http://idi-wolit.com/wolit-24/">
                              <papertitle>Generative Image Reconstruction Using Diffusion Models: A Paradigm Shift from Vanilla Sparsity to Generative Modeling</papertitle>
                          </a>
                        
                          <br> 2024 International Workshop on Learning and Information Theory, August 19-20, 2024
                          [<a href="./talks/wolt24.pdf"target="_blank">slides</a>]
                          <br>
                          <br>


                        
                          <papertitle>Quantized Compressed Sensing with Score-based Generative Models</papertitle>
                          <br> The 52th Asilomar Conference on Signals, Systems, and Computers 2023, Oct 30, 2023
                          [<a href="./talks/Asilomar.pdf"target="_blank">slides</a>]
                          <br>
                          <br>

                        
                        

                          <papertitle>Quantized Compressed Sensing with Score-based Generative Models</papertitle>
                          <br> The SINE Seminar, UIUC, Oct 16, 2023
                          [<a href="./talks/SINE_semina.pdf"target="_blank">slides</a>]
                          <br>
                          <br>

                        
                          <papertitle>Training Binary Neural Networks Using the Bayesian Learning Rule</papertitle>
                          <br> Zhejiang University, Online, July 29, 2022
                          <br>
                          <br>

                          <a href="https://www.koushare.com/lives/room/506610">
                              <papertitle>A Statistical Mechanics Analysis of Ising Model Selection</papertitle>
                          </a>
                          <br> Chinese Academy of Science, Online,  Jan 03, 2022
                          <br>
                          <br>

                          <a href="https://groups.google.com/g/ibisml/c/aCCmFe1JstU?utm_source=dlvr.it&utm_medium=twitter">
                              <papertitle>A High Bias Low Variance Introduction to Approximate Inference</papertitle>
                          </a>
                          <br> Tokyo Institute of Technology,Tokyo, Japan,  Oct 11, 2019
                          [<a href="./talks/TokyoTech_talk_2019.pdf"target="_blank">slides</a>]
                          [<a href="https://github.com/mengxiangming/approximate-inference"target="_blank">code</a>]
                        
                          <br>
                          <br>

                          <a href="https://aip.riken.jp/events/event_86679/?lang=ja}{Approximate Bayesian Inference for Generalized Linear Models">
                              <papertitle>Approximate Bayesian Inference for Generalized Linear Models</papertitle>
                          </a>
                          <br> RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan,  Feb 27,  2019
                          [<a href="./talks/RIKEN_talk_2019.pdf" target="_blank">slides</a>]
                          <br>
                          <br>


                          <a href="http://pil2018.csp.escience.cn/dct/page/65581">
                              <papertitle>A Unified Approximate Bayesian Inference Framework for Generalized Linear Models</papertitle>
                          </a>
                          <br> "Physics, Inference, and Learning", Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing, China,  Oct 31,  2018
                          [<a href="./talks/PIL2018_Meng-2-46.pdf" target="_blank">slides</a>]
                          <br>
                          <br>


                      <p></p>
                  </td>
              </tr>
          </table>


<!--          {\textbf{\href{}{}} \hfill {} \\{\textit{``Physics, Inference, and Learning''}}, }-->


  
         <!-- This part adds Reviewing tasks-->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Professional Service</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">

           <p>
            <papertitle>Conference Area Chairs:</papertitle> NeurIPS (2024), ICLR (2025).

 
            </p>
              
            <p>
            <papertitle>Conference Reviewer:</papertitle> NeurIPS (2019-), ICML (2021-), AAAI (2020-), ICLR(2020-), AISTATS(2022-), ALT(2020-), NeurIPS workshop on Machine Learning and the Physical Sciences (2021-).


            </p>
            <p>
            <papertitle>Journal Reviewer:</papertitle>  Statistics and Computing, IEEE  Journal  on  Selected  Areas  in  Communications (JSAC), IEEE Signal Processing  Letters (SPL), IEEE Communication Letters (CL)
            </p>
            </td>
          </tr>
        </table>


  
        <hr>

  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Misc.</heading>
              <p>
                I love reading, music, and table tennis.
         </p>
            </td>
          </tr>

        </table>
<!--        <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <td width="50%"><img src="12.jpg" alt="prl" width="360" height="216"></td>-->
<!--            <td width="50%"><img src="13.jpg" alt="prl" width="360" height="216"></td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="10">-->
<!--            <td width="100%" align="center">A drone flying in the Caltech Real Weather Wind Tunnel (Neural-Fly project)</td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <td width="50%"><img src="3.jpeg" alt="prl" width="360" height="216"></td>-->
<!--            <td width="50%"><img src="10.jpg" alt="prl" width="360" height="216"></td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="10">-->
<!--            <td width="50%" align="center">Beijing National Stadium</td>-->
<!--            <td width="50%" align="center">Caltech Beckman Auditorium</td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <td width="50%"><img src="5.jpeg" alt="prl" width="360" height="216"></td>-->
<!--            <td width="50%"><img src="6.jpeg" alt="prl" width="360" height="216"></td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="10">-->
<!--            <td width="50%" align="center">Wudaokou, Beijing</td>-->
<!--            <td width="50%" align="center">Tokugawaen, Nagoya, Japan</td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20">-->
<!--            <td width="50%"><img src="1.jpeg" alt="prl" width="360" height="216"></td>-->
<!--            <td width="50%"><img src="8.jpeg" alt="prl" width="360" height="216"></td>-->
<!--        </table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="10">-->
<!--            <td width="50%" align="center">Winter Tsinghua</td>-->
<!--            <td width="50%" align="center">Yosemite, California</td>-->
<!--        </table>-->

        <hr>
  
        <table width="80%" align="left" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="left">
                <font size="2">

                  Based on <a href="https://jonbarron.info/">this website</a> and <a href="https://www.gshi.me">this website</a>.
                  </font>
              </p>
            </td>
          </tr>
        </table>




        <table width="20%" align="right" border="0" cellspacing="0" cellpadding="20">
          
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=fso1QjvGeT_butzpL6aypAOb6OK3tTDx9emYr1ZoY-Q&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
<!--         <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=fso1QjvGeT_butzpL6aypAOb6OK3tTDx9emYr1ZoY-Q"></script> -->
        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
